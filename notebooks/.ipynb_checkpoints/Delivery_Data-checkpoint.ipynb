{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad7654d-8e01-44d0-b843-79beed7d6588",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12096\\1595355522.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# numpy compat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_numpy_dev\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_is_numpy_dev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m from pandas.compat.numpy import (\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mis_numpy_dev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mnp_version_under1p19\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\compat\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# numpy versioning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m from pandas.util._decorators import (  # noqa:F401\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mAppender\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mSubstitution\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcache_readonly\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproperties\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcache_readonly\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m from pandas._libs.tslibs import (\n\u001b[0;32m     15\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\interval.pyx\u001b[0m in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro, probplot\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from patsy import dmatrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import shap\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:\\\\Users\\\\13346\\\\OneDrive\\\\Desktop\\\\delhivery_data.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Preview the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7d2bd-f7cc-4fba-8d76-5f7d740d8d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a5e9a-7739-44d9-aef6-595f2340fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total missing values per column\n",
    "df.isnull().sum()\n",
    "\n",
    "\n",
    "# Step 1: Get columns with missing values\n",
    "null_counts = df.isnull().sum()\n",
    "null_counts = null_counts[null_counts > 0]  # keep only columns with nulls\n",
    "\n",
    "# Step 2: Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "null_counts.plot(kind='bar')\n",
    "plt.title('Columns with Missing Values')\n",
    "plt.xlabel('Column')\n",
    "plt.ylabel('Number of Nulls')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb043ad-f934-4c4a-af37-58325dbc5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Null investigation\n",
    "# Filter rows where either source_name or destination_name is null\n",
    "null_rows = df[df['source_name'].isnull() | df['destination_name'].isnull()]\n",
    "\n",
    "# How many total null rows?\n",
    "print(f\"Total rows with nulls: {len(null_rows)}\")\n",
    "\n",
    "# Preview these rows\n",
    "null_rows[['trip_creation_time', 'source_center', 'destination_center', 'route_type']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97355c3b-d228-4173-9a61-c36af781ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 551 null rows have the exact same trip_creation_time and are clustered around a few specific source/destination centers.\n",
    "# This strongly suggests:These nulls are likely from a single batch of trips created at the same time (possibly a system glitch or incomplete logging).\n",
    "#The centers are also clustered (IND342601AAA, IND342902A1B, etc.),The route_type is consistently FTL\n",
    "\n",
    "null_rows['trip_creation_time'].nunique()\n",
    "\n",
    "null_rows['trip_creation_time'].value_counts().head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444afcba-ae9b-4046-9d73-189511184e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(null_rows['source_center'].nunique(), \"unique source centers\")\n",
    "print(null_rows['destination_center'].nunique(), \"unique destination centers\")\n",
    "\n",
    "null_rows['source_center'].value_counts()\n",
    "null_rows['destination_center'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697fff1e-6366-422f-9262-a9930f2d0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Null Investigation Conclusion: Nulls Are Not Random: They are clustered in batches — e.g., 14 records each created at the exact same second.There are only 33 unique source centers and 29 unique destination centers affected. \n",
    "#One source center (IND282002AAD) alone has 151 null rows.Others follow in similarly sized clusters.\n",
    "#Possible cause: The nulls likely stem from system-level issues such as a failed API call or a failed batch process. \n",
    "\n",
    "#Fix for this issue is to use ID's instead of Names-Drop Names\n",
    "df = df.drop(['source_name', 'destination_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f2d7a1-17b7-423c-86cd-4a89d2173b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape: {df.shape}\")\n",
    "df.describe(include='all').T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a309184-0ce0-45b4-829f-cb3b17c8e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Variables Analysis\n",
    "# Create binary target feature is_delayed\n",
    "df['is_delayed'] = (df['actual_time'] > df['osrm_time']).astype(int)\n",
    "\n",
    "#Create continuous target feature delay_minutes\n",
    "df['delay_minutes'] = df['actual_time'] - df['osrm_time']\n",
    "\n",
    "# Preview\n",
    "df[['actual_time', 'osrm_time', 'delay_minutes', 'is_delayed']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685559a8-2253-4aa9-a98e-cc096788cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Datetime\n",
    "df['trip_creation_time'] = pd.to_datetime(df['trip_creation_time'])\n",
    "df['od_start_time'] = pd.to_datetime(df['od_start_time'])\n",
    "df['od_end_time'] = pd.to_datetime(df['od_end_time'])\n",
    "\n",
    "# Extract time-based features\n",
    "df['trip_hour'] = df['trip_creation_time'].dt.hour\n",
    "df['trip_dayofweek'] = df['trip_creation_time'].dt.day_name()\n",
    "df['trip_month'] = df['trip_creation_time'].dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d28de-1f5f-4ba8-8c34-7c7d5221cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary stats\n",
    "df['delay_minutes'].describe()\n",
    "\n",
    "# Boxplot\n",
    "plt.figure(figsize=(10, 2))\n",
    "sns.boxplot(x=df['delay_minutes'])\n",
    "plt.title('Boxplot of Delay Minutes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa264b1-1aa0-48f5-8547-505e17b9b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have extreme right skew in our continuous variable delay_minutes. The upper bound of the IQR range is 500-700 minutes and everything beyond this is considered an outlier. Next step is to cap the outliers using Winsorization. \n",
    "# Calculate IQR\n",
    "q1 = df['delay_minutes'].quantile(0.25)\n",
    "q3 = df['delay_minutes'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Bounds\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "# Cap values\n",
    "df['delay_minutes_capped'] = df['delay_minutes'].clip(lower=lower_bound, upper=upper_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c9ea81-1374-42ac-8c73-87f5bfcc52a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 2))\n",
    "sns.boxplot(x=df['delay_minutes_capped'])\n",
    "plt.title('Boxplot of Delay Minutes (Capped)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ed181-127d-4774-97c3-4c5023f79ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking normality of capped values\n",
    "\n",
    "# Histogram\n",
    "df['delay_minutes_capped'].hist(bins=50)\n",
    "plt.title(\"Histogram of Capped Delay Minutes\")\n",
    "plt.show()\n",
    "\n",
    "# Q-Q plot\n",
    "probplot(df['delay_minutes_capped'].dropna(), dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q Plot of Capped Delay Minutes\")\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk test (use sample if dataset is large)\n",
    "sample = df['delay_minutes_capped'].dropna().sample(3000, random_state=42)\n",
    "stat, p = shapiro(sample)\n",
    "print(f\"Shapiro-Wilk Test: Stat={stat:.3f}, p={p:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa17ce0-4738-44f0-996b-a548a90f3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Even after the initial capping, there is still a right skew. The spike at 600 is from winsorization. The Q-Q plot confirms non-normal distribution, even after capping. \n",
    "#Next Step is Log Transformation dur to the extreme right skew and  extreme outliers.\n",
    "\n",
    "# Only apply log1p to valid (non-negative) values\n",
    "df['delay_log'] = df['delay_minutes_capped'].apply(lambda x: np.log1p(x) if x >= 0 else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d161150b-4fac-42a5-bbdb-90597cf81768",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 2))\n",
    "sns.boxplot(x=df['delay_log'])\n",
    "plt.title('Boxplot of Capped Delay Minutes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce772a6-bbbe-4bff-a8b8-0da6bf2cf550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Drop delay-related columns (already analyzed)\n",
    "cols_to_plot = numeric_cols.drop(['delay_minutes', 'delay_log']) if 'delay_log' in numeric_cols else numeric_cols.drop(['delay_minutes'])\n",
    "\n",
    "# Set up the figure\n",
    "fig, axes = plt.subplots(nrows=len(cols_to_plot) // 2 + 1, ncols=2, figsize=(15, 4 * (len(cols_to_plot)//2 + 1)))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(cols_to_plot):\n",
    "    sns.histplot(df[col].dropna(), ax=axes[i], kde=True)\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "    axes[i].set_xlabel('')\n",
    "    \n",
    "# Hide any unused plots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4bb9a-5377-49d9-b27e-ea68376537b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Even though there continues to be some skewnewss in the variables, I am going to continue with EDA and look at a correlation heatmap due to the possibility of linear relationships between variales. \n",
    "# Select only numeric features for correlation\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = numeric_cols.corr()\n",
    "\n",
    "# Set up the heatmap\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Correlation Heatmap of Numerical Features')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed80ca6-a9ec-47ff-b8b6-e51ef46ae567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Key observations: Highly Correlated Features (Correlation > 0.9): cutoff_factor, actual_distance_to_destination, osrm_time, osrm_distance, and actual_time are almost perfectly correlated.\n",
    "#Most likely do not need all of these in  model—might introduce multicollinearity.\n",
    "# Low Correlation Features:trip_hour and trip_month show very weak correlation with the target. They might still hold some interaction effects but won't help much as standalone predictors.\n",
    "# Target Feature:  delay_minutes is the base target,  already engineered delay_minutes_capped and delay_log, which show slightly reduced but still strong correlation (0.87 and 0.75, respectively). That’s expected due to transformation and capping.\n",
    "\n",
    "#Next steps: Create Deltas/Ratios\n",
    "#Time Delta\n",
    "df['time_diff_vs_osrm'] = df['actual_time'] - df['osrm_time']\n",
    "\n",
    "#Distance Delta\n",
    "df['distance_diff_vs_osrm'] = df['actual_distance_to_destination'] - df['osrm_distance']\n",
    "\n",
    "#Efficiency Ratio\n",
    "df['time_per_km'] = df['actual_time'] / df['actual_distance_to_destination']\n",
    "\n",
    "#Performance Ratio\n",
    "df['osrm_time_ratio'] = df['actual_time'] / df['osrm_time']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6429420b-2071-406a-b8c2-7c95f4158cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize new features\n",
    "# Select engineered features\n",
    "engineered_features = ['time_diff_vs_osrm', 'distance_diff_vs_osrm', 'time_per_km', 'osrm_time_ratio']\n",
    "\n",
    "# Plot distributions\n",
    "for feature in engineered_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df[feature], kde=True, bins=50)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af699da-c591-456c-bb70-82eb577c699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with delay\n",
    "df[engineered_features + ['delay_log']].corr()['delay_log'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f62354-3a63-4e65-b24f-a2cc6b60d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The new engineered features have strong correlations with delay_log: \n",
    "# time_diff_vs_osrm: 0.75 → strong, as expected. This feature likely reflects actual delays most directly.\n",
    "# distance_diff_vs_osrm: -0.67 → strong inverse. Possibly longer distances are better planned? Or this flags underestimation.\n",
    "# osrm_time_ratio: 0.18 → weak, but intuitive: how far off the trip time is from expectations.\n",
    "# time_per_km: 0.09 → very weak; not informative alone.\n",
    "\n",
    "# Next Steps: Feature Selection\n",
    "# Drop Identifiers: route_schedule_uuid, trip_uuid\n",
    "# Redundant time columns (already created deltas/ratios):actual_time, osrm_time, start_scan_to_end_scan\n",
    "# Redundant distance columns: actual_distance_to_destination, osrm_distance\n",
    "# Raw delay_minutes(using delay_log now)\n",
    "# Other Columns :trip_creation_time, od_start_time, od_end_time (already extracted hour/month), cutoff_timestamp, is_cutoff, cutoff_factor, factor, segment_factor (unknown meaning)\n",
    "columns_to_drop = [\n",
    "    'route_schedule_uuid', 'trip_uuid',\n",
    "    'trip_creation_time', 'od_start_time', 'od_end_time',\n",
    "    'actual_time', 'osrm_time', 'start_scan_to_end_scan',\n",
    "    'actual_distance_to_destination', 'osrm_distance',\n",
    "    'delay_minutes', 'delay_minutes_capped',\n",
    "    'cutoff_timestamp', 'is_cutoff', 'cutoff_factor',\n",
    "    'factor', 'segment_factor'\n",
    "]\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e2e66-253d-442e-9bd2-5abacbe846aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric features (excluding target 'delay_log')\n",
    "numeric_df = df.select_dtypes(include=['number']).drop(columns=['delay_log'])\n",
    "\n",
    "# Build design matrix for predictors only\n",
    "X_vif = dmatrix(\"+\".join(numeric_df.columns), data=df, return_type='dataframe')\n",
    "\n",
    "# Calculate VIF\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df[\"feature\"] = X_vif.columns\n",
    "vif_df[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "vif_df = vif_df.sort_values(by='VIF', ascending=False)\n",
    "vif_df\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61b3ebca-3896-46d8-94fe-9b1f77ebd13f",
   "metadata": {},
   "source": [
    "### Retained Features – Statistical Summary\n",
    "\n",
    "| Feature                  | Mean    | Std Dev | Min     | Max     | VIF     | Description                                                                   |\n",
    "|--------------------------|---------|---------|---------|---------|---------|-------------------------------------------------------------------------------|\n",
    "| `segment_osrm_time`      | ~7.39   | ~6.30   | 0       | 78.33   | **12.48** | OSRM-estimated time (in minutes) for trip segment.                          |\n",
    "| `time_per_km`            | ~1.44   | ~2.44   | 0       | 139.13  | **7.89**  | Actual time per km of the trip. Indicates efficiency.                       |\n",
    "| `osrm_time_ratio`        | ~1.07   | ~1.46   | 0       | 76.76   | **7.66**  | Ratio of actual to OSRM time — values >1 suggest delays.                    |\n",
    "| `time_diff_vs_osrm`      | ~62.44  | ~182.61 | -1091.9 | 3130.5  | **4.33**  | Difference between actual and OSRM time. Positive = delay.                  |\n",
    "| `distance_diff_vs_osrm`  | ~-18.55 | ~60.56  | -489.0  | 180.5   | **4.18**  | Difference between actual and OSRM distance. Mostly negative.               |\n",
    "| `segment_actual_time`    | ~6.61   | ~7.28   | 0       | 93.83   | **2.13**  | Actual segment delivery time (minutes).                                     |\n",
    "| `is_delayed`             | 0.20    | 0.40    | 0       | 1       | **1.06**  | Binary flag: 1 = delayed trip.                                              |\n",
    "| `trip_hour`              | ~11.96  | ~6.97   | 0       | 23      | **1.00**  | Hour of day when trip was created.                                          |\n",
    "| `trip_month`             | ~9.11   | ~0.32   | 9       | 10      | **1.00**  | Month of trip (mostly September/October).                                   |\n",
    "\n",
    "# Feature selection was guided by both domain relevance and statistical diagnostics. The retained variables reflect temporal, spatial, and operational dynamics of logistics performance, including engineered features that quantify deviations between actual and expected trip conditions (e.g., `time_diff_vs_osrm`, `osrm_time_ratio`). Multicollinearity was assessed using Variance Inflation Factor (VIF); features with VIF > 10 were considered redundant and removed to stabilize model estimates and improve interpretability. The final feature set maintains predictive power while reducing the risk of overfitting and ensuring meaningful coefficient attribution in downstream modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e981d-301c-4cd5-a982-8c7f404d8098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "# Drop rows with missing target values\n",
    "df_model = df.dropna(subset=['delay_log'])\n",
    "\n",
    "# Redefine X and y using the cleaned data\n",
    "X = df_model[[\n",
    "    'segment_osrm_time',\n",
    "    'time_per_km',\n",
    "    'osrm_time_ratio',\n",
    "    'time_diff_vs_osrm',\n",
    "    'distance_diff_vs_osrm',\n",
    "    'segment_actual_time',\n",
    "    'is_delayed',\n",
    "    'trip_hour',\n",
    "    'trip_month'\n",
    "]]\n",
    "\n",
    "y = df_model['delay_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15721eca-8a0e-4324-96a1-c2f27bb820fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db078438-f1b5-49ab-8414-c1257d5895d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Linear Regression Model \n",
    "# Initialize and fit model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lr_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba785dfb-efeb-4679-92e1-451d36024ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\" Mean Absolute Error (MAE):     {mae:.2f}\")\n",
    "print(f\" Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\" R-squared (R² Score):            {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28793ea-838a-4de9-b62b-7c95a95e9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred)\n",
    "plt.xlabel(\"Actual Delay (log scale)\")\n",
    "plt.ylabel(\"Predicted Delay (log scale)\")\n",
    "plt.title(\"Predicted vs. Actual Delays\")\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], '--', color='gray')  # Reference line\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361fd133-4bef-4df8-9de0-11be1157f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation of results: # Mean Absolute Error (MAE = 0.73): \n",
    "# On average, the predicted log delay deviates from the actual log delay by 0.73. In the original (non-log) space, this translates to multiplicative error (as log-transformed targets are exponential when reversed). \n",
    "# Root Mean Squared Error (RMSE = 0.89): # Slightly higher than MAE, indicating some influence from outliers or high-variance samples, though not dramatically. # R² Score (0.62):The model explains 62% of the variance in the delay (on log scale), which is solid for a linear model in a logistics context with natural variability. # Scatterplot Insights: # Predictions track the general trend well. \n",
    "# There's compression at the lower end (under-prediction of smaller delays). # At high actual delays, there's some underestimation and spread, indicating the model struggles with extremes (as expected with linear models).\n",
    "\n",
    "# Scatterplot Insights:\n",
    "# The predicted vs. actual plot shows a clear upward trend, confirming the model captures the general delay structure.\n",
    "# The lower-left compression suggests bias toward overprediction for small actual delays (log values < 2).\n",
    "# AAs actual delays increase, prediction variance widens, and a tendency to underpredict extreme delays emerges — a common limitation of linear models under non-linear or heteroscedastic target behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faaeb13-a5c5-40fd-bcfa-d848fed79c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residual Diagnostics\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# 1. Residuals vs Fitted values\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.scatterplot(x=y_pred, y=residuals)\n",
    "plt.axhline(0, linestyle='--', color='red')\n",
    "plt.title('Residuals vs Predicted Values')\n",
    "plt.xlabel('Predicted Delay (log scale)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "# 2. Q-Q plot to check normality\n",
    "sm.qqplot(residuals, line='45', fit=True)\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.show()\n",
    "\n",
    "# 3. Histogram of residuals\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(residuals, kde=True, bins=50)\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "# 4. Residuals vs Actual values\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.scatterplot(x=y_test, y=residuals)\n",
    "plt.axhline(0, linestyle='--', color='red')\n",
    "plt.title('Residuals vs Actual Values')\n",
    "plt.xlabel('Actual Delay (log scale)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "# 5. Durbin-Watson Test for Autocorrelation\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "dw_stat = durbin_watson(residuals)\n",
    "print(f\"Durbin-Watson statistic: {dw_stat:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b858636e-8973-4a89-a7e5-e17e32587ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Analysis\n",
    "# Residual vs. Predicted Values: Clear funnel shape: Residual variance increases with predicted delay values. This suggests heteroscedasticity, which violates a key OLS assumption (constant variance of errors). Non-random pattern: Indicates that the model isn’t capturing non-linear relationships well, especially for higher delays. \n",
    "# Q-Q Plot: Residuals deviate from the red line at both tails, especially the lower end. This shows non-normality of residuals. Normality assumption is violated, especially in the lower quantiles — likely due to outliers or left-skew.\n",
    "# The histogram is right-skewed with a long left tail. There’s a pile-up near zero (as expected with log-transformed targets), but also extreme negative residuals, which explain underprediction in some cases.\n",
    "# There is a visible pattern rather than a random scatter, especially at the extremes. The residuals appear to become more negative at higher actual values, another sign of underfitting in extreme delay cases.\n",
    "\n",
    "# Statistical Implications: \n",
    "# Heteroscedasticity: Violates one of the Gauss-Markov assumptions, reducing the efficiency of coefficient estimates. \n",
    "# This can be mitigated in future models by: Transforming variables (e.g. Box-Cox, log features); using models that don’t assume constant variance such as gradient boosting or random forests; or applying robust regression techniques.\n",
    "# Non-normality of residuals: Mostly important for inference(confidence intervals, p-values), which may not be critical, but still affects interpretability and reliability.\n",
    "# Model Bias: The residuals show systematic underprediction at higher values — this indicates that a linear model may be too simplistic for capturing the true functional form.\n",
    "\n",
    "# While linear regression provides a useful baseline, residual diagnostics reveal significant violations of its assumptions, namely heteroscedasticity, non-normality of errors, and inability to capture non-linear relationships. These issues suggest that a linear model is not sufficient to model the complexity and variability inherent in real-world logistics delays, especially in edge cases and long-tail events. Consequently, a more flexible and robust modeling approach is warranted.\n",
    "\n",
    "# Given the presence of both linear global trends and complex, nonlinear interactions within the logistics delay data, a hybrid ensemble model enables us to capture the strengths of both parametric and non-parametric learners. Linear models offer interpretability and efficiency for additive effects, while tree-based models such as Random Forests and XGBoost effectively handle feature interactions, non-linearity, and heteroscedasticity. By stacking these models in an ensemble, we reduce bias and variance simultaneously, achieving improved generalization performance in high-dimensional, noisy environments typical of supply chain systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e4ef9-382a-4f0b-b298-37e429bec98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'segment_osrm_time',        # Estimated trip time\n",
    "    'segment_actual_time',      # Actual segment time (optional — less leaky)\n",
    "    'time_per_km',              # Basic efficiency\n",
    "    'is_delayed',               # Binary signal\n",
    "    'trip_hour',                # Temporal pattern\n",
    "    'trip_month'                # Seasonal trend\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb2460-17ab-4ccd-95b0-a5f76639de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with valid target\n",
    "df_model = df.dropna(subset=['delay_log'])\n",
    "\n",
    "# Features and target\n",
    "X = df_model[features]\n",
    "y = df_model['delay_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28318244-8a7b-46c0-adf9-4b985d011c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10449db-af97-4d2b-950e-6c26f4b0ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model):\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "\n",
    "models = {\n",
    "    'Random Forest': build_model(RandomForestRegressor(random_state=42)),\n",
    "    'Gradient Boosting': build_model(GradientBoostingRegressor(random_state=42)),\n",
    "    'XGBoost': build_model(XGBRegressor(random_state=42)),\n",
    "    'Voting Ensemble': VotingRegressor([\n",
    "        ('rf', RandomForestRegressor(random_state=42)),\n",
    "        ('gb', GradientBoostingRegressor(random_state=42)),\n",
    "        ('xgb', XGBRegressor(random_state=42))\n",
    "    ]),\n",
    "    'Stacking Ensemble': StackingRegressor(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestRegressor(random_state=42)),\n",
    "            ('gb', GradientBoostingRegressor(random_state=42)),\n",
    "            ('xgb', XGBRegressor(random_state=42))\n",
    "        ],\n",
    "        final_estimator=LinearRegression(),\n",
    "        passthrough=True\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63303f-8d3b-4b2e-8ab6-042c043ce799",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    \n",
    "    # Use pipeline if available\n",
    "    if isinstance(model, Pipeline):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        # Voting and Stacking (not in pipeline)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\" MAE:  {mae:.3f}\")\n",
    "    print(f\" RMSE: {rmse:.3f}\")\n",
    "    print(f\" R²:   {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4ce57-5faf-40a2-9401-f789759458d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance Summary\n",
    "\n",
    "#| Model                 | MAE   | RMSE  | R²        |\n",
    "#| --------------------- | ----- | ----- | --------- |\n",
    "#| **Random Forest**     | 0.753 | 0.987 | 0.538     |\n",
    "#| **Gradient Boosting** | 0.854 | 1.060 | 0.467     |\n",
    "#| **XGBoost**           | 0.753 | 0.960 | 0.563     |\n",
    "#| **Voting Ensemble**   | 0.770 | 0.967 | 0.557     |\n",
    "#| **Stacking Ensemble** | 0.742 | 0.951 | **0.571** |\n",
    "\n",
    "# Interpretation\n",
    "# XGBoost and Random Forest are the top-performing base models.\n",
    "# Stacking Ensemble achieved the lowest MAE/RMSE and highest R² — indicating it best captured both low and high delays.\n",
    "# Gradient Boosting lagged behind, possibly due to default parameter limitations.\n",
    "# Voting Ensemble improves stability but slightly smooths extremes.\n",
    "\n",
    "# Stacking Ensemble is currently your best model, balancing bias and variance well.\n",
    "# The R² ≈ 0.57 suggests ~57% of delay variance (in log-scale) is explained — a strong result in noisy logistics datasets.\n",
    "# MAE and RMSE in the 0.74–0.95 range shows good average prediction error — manageable at the operational level.\n",
    "# The error gap between base models and ensemble confirms the value of combining predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce650cc-9c03-473c-9b84-e2dda703dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_models = {\n",
    "    'Random Forest': models['Random Forest'],\n",
    "    'Gradient Boosting': models['Gradient Boosting'],\n",
    "    'XGBoost': models['XGBoost']\n",
    "}\n",
    "\n",
    "# Feature importances from tree models in pipelines\n",
    "for name, pipeline in tree_models.items():\n",
    "    regressor = pipeline.named_steps['regressor']\n",
    "    importances = regressor.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.title(f'Feature Importances - {name}')\n",
    "    plt.bar(range(len(importances)), importances[indices])\n",
    "    plt.xticks(range(len(importances)), X_train.columns[indices], rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e4bc0-ca7d-43fd-8dcb-7fc06ce78d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "# | Feature               | Random Forest             | Gradient Boosting         | XGBoost              | Notes                                                                |\n",
    "# | --------------------- | ------------------------- | ------------------------- | -------------------- | -------------------------------------------------------------------- |\n",
    "# | `time_per_km`         | **Most important** (≈39%) | **Most important** (≈35%) | Mid-level            | Proxy for delivery efficiency; dominant across tree ensembles.       |\n",
    "# | `segment_actual_time` | High (≈34%)               | High (≈32%)               | Medium-high          | Captures real delivery duration — naturally correlated with delay.   |\n",
    "# | `segment_osrm_time`   | Medium (≈14%)             | Medium (≈26%)             | Lower                | Baseline time estimate; interaction effects possible.                |\n",
    "# | `is_delayed`          | Low (≈2%)                 | Low (≈4%)                 | **Very High** (≈59%) | XGBoost heavily leverages binary flag — nonlinear signal extraction. |\n",
    "# | `trip_hour` / `month` | Low importance            | Negligible                | Low                  | Time-based features don't show strong standalone predictive power.   |\n",
    "\n",
    "# Random Forest and Gradient Boosting prioritize efficiency (time_per_km) and actual time.\n",
    "# XGBoost stands out by heavily leveraging is_delayed — highlighting how its boosting mechanism captures non-linear patterns from binary flags that linear models miss.\n",
    "# Trip metadata (trip_hour, trip_month) consistently has low importance — possibly due to lack of variability or weaker interactions with delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c912ad4-f442-43fe-b301-e9735c257847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TreeExplainer for any tree-based model (e.g., XGBoost, RF, GB)\n",
    "explainer = shap.Explainer(models['XGBoost'].named_steps['regressor'])\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Summary plot (global feature impact)\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "\n",
    "# Beeswarm plot (per-instance contributions)\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "# Optional: Explain a specific prediction\n",
    "i = 10  # row index\n",
    "shap.plots.waterfall(shap_values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa107d72-e3ad-4808-9661-3a9088bc805f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
